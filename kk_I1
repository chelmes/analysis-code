#!/usr/bin/python
################################################################################
#
# Author: Christopher Helmes (helmes@hiskp.uni-bonn.de)
# Date:   Februar 2015
#
# Copyright (C) 2015 Christopher Helmes
# 
# This program is free software: you can redistribute it and/or modify it under 
# the terms of the GNU General Public License as published by the Free Software 
# Foundation, either version 3 of the License, or (at your option) any later 
# version.
# 
# This program is distributed in the hope that it will be useful, but WITHOUT 
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS 
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with tmLQCD. If not, see <http://www.gnu.org/licenses/>.
#
################################################################################
#
# Function: Pi-Pi-scattering analysis in the center of mass frame 
#
# For informations on input parameters see the description of the function.
#
################################################################################

# system imports
import os.path as osp
from scipy import stats
import numpy as np
from numpy.polynomial import polynomial as P
import matplotlib
matplotlib.use('QT4Agg') # has to be imported before the next lines
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

# Christian's packages
import analysis as ana
def main():
    # number of bootstrap samples
    nbsamples = 1500
    # temporal lattice extent
    tmin = 1
    T = 48
    L = 24
    # Strange quark masses and unitary kaon mass
    #mk_unit = 0.26695
    #dmk_unit = 0.00052
    amu_s = [0.0185,0.0225,0.02464]
    p_zero = np.asarray([0,0,0])
    #amu_s = [0.0185]
    #ensemblelist = ['A40.24/','A60.24/','A80.24/','A100.24/']
    ensemblelist = ['A60.24/']
    filelist = ['k_corr_p0.dat','kk_A1_TP0_00.dat']
    #filelist = ['C2.dat','kpi_4.dat']
    pathroot ="/hiskp2/helmes/k-k-scattering/"
    for ens in ensemblelist:
        datapath = pathroot+"data/"+ens
        cachepath = pathroot+"data/cache/"
        #print datapath
        #Numpy array to hold all data at end
        m_k_dat = [] #np.zeros((len(amu_s), nbsamples))
        a_k_dat = [] #np.zeros((len(amu_s), nbsamples))
        m_k_unit =[] # np.zeros(nbsamples)
        save_res=open(datapath+"res.dat","w")
        save_res.write("amu_s\tm_k d_stat(m_k) +d_sys(m_k) -d_sys(m_k)\t delE d_stat(delE) +d_sys(delE) -d_sys(delE)\ta_kk d_stat(a_kk) +d_sys(a_kk) -d_sys(a_kk)\tm_k*a_kk d_stat(m_k*a_kk) +d_sys(m_k*a_kk) -dsys(m_k*a_kk)\n") 
        for s in range(0,len(amu_s)):
            # need two cachepaths
            name_cache_k = cachepath+ens[:-1]+"_"+str(amu_s[s])[3:]+"_"+filelist[0]
            name_cache_kk = cachepath+ens[:-1]+"_"+str(amu_s[s])[3:]+"_"+filelist[1]
            #set up paths for data read in and result storage
            path = datapath+"amu_s_"+str(amu_s[s])[3:]+"/"
            #path = datapath
            #tmp_out = pathroot+"plots/"+ens+"amu_s_"+str(amu_s[s])[3:]
            tmp_out = pathroot+"plots/"
#            pfit = PdfPages("%s%sk_plot.pdf" % (tmp_out,"_corr_")) 
#            prat = PdfPages("%s%skk_plot.pdf" % (tmp_out,"_ratio_")) 
            # start with usual analysis (thermal states contamination expected)
            fitfunc = lambda p,t: 10e6 if p[1] < 0.0 else 0.5*p[0]*p[0]*(np.exp(-p[1]*t) + np.exp(-p[1]*(T-t)))
            par_mass = [1.]
            par_ratio = [2.,0.06]
            massfunc = lambda p, t: p
            
            T2=int(T/2)+1
            tlist_corr = np.linspace(0., float(T2), float(T2), endpoint=False)
            tlist_ratio = np.linspace(0.5, float(T2-0.5), float(T2-1), endpoint=False)


            #-------------------- data input ----------------------------

            # Check if data is already cached
            test_k = name_cache_k+".npy"
            test_kk = name_cache_kk+".npy"
            if osp.isfile(test_k) and osp.isfile(test_kk):
                #print("reusing samples %s and\n%s" % (test_k, test_kk))
                bsamples_k = ana.read_data(test_k)
                bsamples_kk = ana.read_data(test_kk)
            else:
            # else read data anew
                print("Starting from scratch")
                corrs_k = ana.read_data_ascii("".join((path, filelist[0])))
                corrs_kk = ana.read_data_ascii("".join((path, filelist[1])))
                nbcfg = corrs_k.shape[0]
                # Create symmetrized bootstrap samples of 2pt and 4pt Corr-function
                bsamples_k = ana.sym_and_boot(corrs_k, nbsamples)
                bsamples_kk = ana.sym_and_boot(corrs_kk, nbsamples)
                # Cache Bootstrapsamples using tested Cachename
                ana.write_data(bsamples_k, name_cache_k)
                ana.write_data(bsamples_kk, name_cache_kk)
                print("Samples cached in %s" % cachepath)

            # Check mean correlators
            corr_k_data, d_k_data = ana.calc_error(bsamples_k)
            corr_kk_data, d_kk_data = ana.calc_error(bsamples_kk)


            #------------------- m_k from bsamples_k --------------------------

            label = ["Kaon m_eff", "time", "C(t)", "data", ""]
            up=23

            ## due to small variance changes can fit single correlators to the end
            #mass, mmass, dmass = ana.compute_mass(bsamples_k, True)
            ## ana.scan_fit_range(massfunc, tlist_corr,mass, par_mass)
            ## TODO: set two points more than wanted for upper limit
            #ranges = ana.set_fit_intervall(mass, [10], [24], 10)
            #
            ## For mass use one parameter fit
            ## res has shape (nb_samples, nb_pars, ncorr, nb_fitranges)
            #print ens[:-1], amu_s[s]
            #prefix = "_"+ens[:-1]+str(amu_s[s])[3:]
            #res, chi2, pval = ana.genfit(mass,ranges,massfunc,
            #    par_mass,1,prefix,np.asarray([0,0,0]),label,tmp_out,"mass",False)
            #
            #print("Shape of mass calculation is:")
            #print res.shape
            #np.append(m_k_dat,res)
            #prefix = ens[:-1]+"_mass_"+str(amu_s[s])[3:] 
            #m_k_w, dm_stat, dm_sys, weights_m_k = ana.sys_error(res[:,0],pval,
            #                                                    p_zero,prefix,
            #                                                    tmp_out)
            #print("m_k +/- d_stat(m_k) + d_sys(m_k) -d_sys(m_k)\n%f +/- %f +%f-%f\n" 
            #    %(m_k_w, dm_stat, dm_sys[0], dm_sys[1]))
            ##print("weights_m_k shape is:%d" % weights_m_k.shape)

            #--------------- Have a look at the 4 pt mean correlator and effective mass -----------

            label = ["KK 4pt func", "time", "C4(t)", "data", ""]
            up=23
            pfit = PdfPages("%s%sC4_%s_plot.pdf" % (tmp_out,"m_eff_",str(amu_s[s]))) 
            ana.corr_fct_with_fit(tlist_corr, corr_kk_data, d_kk_data, massfunc, [0], [1,T2-2], label, pfit, logscale=True, setLimits=False)

            label[2] = "m_eff_4pt(t)"
            m_eff_C4, mm_eff_C4, dm_eff_C4 = ana.compute_mass(bsamples_kk, False)
            ana.corr_fct_with_fit(tlist_corr, mm_eff_C4, dm_eff_C4, massfunc,
                [0], [1,T2-2], label, pfit, False, False)
            pfit.close()

            #-------------- Ratio of C_kk and C_k^2 ---------------------------

            #R_boot, R_mean, R_stdev, = ana.simple_ratio_subtract(bsamples_kk, bsamples_k, bsamples_k)
            #label = ["Ratio K-K", "time", "R(t)", "data", ""]
            ## define the ratiofunction, Using fitted energies seems to make more
            ## sense
            #ratio = lambda p, t, e : p[0]*(np.cosh(p[1]*(t-T/2))+np.sinh(p[1]*(t-T/2))/(np.tanh(2*e*(t-T/2))))
            ## define fit ranges
            #ranges1 = ana.set_fit_intervall(R_boot,[13],[25],8)
            ## res_R has shape (nb_samples, nb_pars, n_corr, nb_ranges1 *
            ## nb_ranges2)
            #res_R, chi2_R, pvals_R = ana.genfit_comb(R_boot,ranges1,ranges,
            #                                         ratio,par_ratio,res,
            #                                         1,ens[:-1],label,tmp_out,
            #                                         "ratio", verbose=False)
            ## Reshape res_R and pvals_R for systematic error calculation
            #r = len(ranges[0])*len(ranges1[0]) 
            ##print res_R.size, pvals_R.size
            ##print r, nbsamples 
            #w_res_R = np.reshape(res_R, (nbsamples,2,1,r))
            #w_pvals_R = np.reshape(pvals_R, (nbsamples,1,r))
            #print("Shape of del_E calculation is:")
            #print res_R.shape, pvals_R.shape
            #prefix = ens[:-1]+"ratio_"+str(amu_s[s])[3:]
            #del_E_w, ddel_E_stat, ddel_E_sys , weights_del_E = ana.sys_error(w_res_R[:,1], w_pvals_R, p_zero,prefix,tmp_out)
            #print("delE +/- d_stat(del_E) + d_sys(del_E) -d_sys(del_E)\n%f +/- %f +%f-%f\n" 
            #    %(del_E_w, ddel_E_stat, ddel_E_sys[0], ddel_E_sys[1]))
            ##print("weights_delE shape is:%d" % weights_m_k.shape)
            #
 #          # prat.close()
            ##print("results shapes:")
            ##print res_R.shape, res.shape

          #  #---------------------- Scattering length a_kk --------------------

        #    # calculate Scattering length
        #    a_kk = ana.calc_scat_length(w_res_R[:,1],res,L)
        #    # and its median and errors and weights
        #    print a_kk.shape, weights_del_E.shape
        #    # TODO: reshape is nasty but necessary...
        #    #a_kk = np.atleast_3d(a_kk)
        #    #weights_del_E = np.atleast_2d(weights_del_E)
        #    #weights_del_E.shape = (9,1)
        #    #print a_kk[0]
        #    #a_kk.shape = (1500,1,9)
        #    #print a_kk[0]
        #    a_kk_w, a_kk_stat, a_kk_sys = ana.sys_error_der(a_kk, weights_del_E ,ens[:-1])
        #    #print("%f\t%f +/- %f\t%f +/- %f\t%f +/- %f\t%f +/- %e"%(amu_s[s],
        #    #  m_k_w, dm_stat, dm_sys, del_E_w, ddel_E_stat, ddel_E_sys,
        #    #  a_kk_w, a_kk_stat, a_kk_sys[0], a_kk_sys[1]
        #    #  #np.median(m_k_akk[:,0]), np.std(m_k_akk[:,0]
        #    #  ))
        #    #save_res.write("%f\t%f +/- %f +%f-%f\t%f +/- %f +%f-%f\t%f +/- %f +%f-%f\n"%(amu_s[s],
        #    #  m_k_w, dm_stat[0], dm_sys[0], dm_sys[1], del_E_w, ddel_E_stat[0],
        #    #  ddel_E_sys[0],ddel_E_sys[1],
        #    #  a_kk_w, a_kk_stat, a_kk_sys[0], a_kk_sys[1]))
        #    
        #    #------------------------- m_k*a_kk -------------------------------
        #    # To match the shapes the tiling is done by the second fit range
        #    ma_k = a_kk * np.tile(res[:,0,0],len(ranges1[0]))
        #    weights_ma_k = weights_del_E * np.tile(weights_m_k,len(ranges1[0])) 
        #    ma_k_w , ma_k_stat, ma_k_sys = ana.sys_error_der(ma_k, weights_ma_k,
        #        ens[:-1])

        #    save_res.write("%f\t%f %f +%f-%f\t%f +/- %f +%f-%f\t%f +/- %f +%f-%f\t%f +/- %f +%f-%f\n"
        #      %( amu_s[s], m_k_w, dm_stat[0], dm_sys[0], dm_sys[1],
        #      del_E_w, ddel_E_stat[0], ddel_E_sys[0],ddel_E_sys[1],
        #      a_kk_w, a_kk_stat, a_kk_sys[0], a_kk_sys[1],
        #      ma_k_w, ma_k_stat, ma_k_sys[0], ma_k_sys[1]))
        #
        #save_res.close()

        ##----------------- Start analysis ------------------------------------------

        ## reshape data by switching axes
        #a_k_fin = a_k_dat.T
        #m_k_fin = m_k_dat.T

        ## Unitary mass matching of amu_s
        ## fit samples to straight line
        ##linear fitfunction
        #par_match = [0.1,2.]
        #m_k2 = lambda p, mu : p[0]*mu+p[1]
        #res_m2, chi2_m2, pvals_m2 = ana.fitting(m_k2,
        #    np.asarray(amu_s)[1:], m_k_fin[:,1:]**2, par_match, None, False, False)

        ## Create parametrized bootstrap samples for unitary mass
        #mk_u = np.random.normal(mk_unit, dmk_unit, len(corrs_k[0]))
        #m_k_unit = ana.bootstrap(mk_u, nbsamples)
        ## extract nb_samples amu_s values
        #amu_s_fin = (m_k_unit**2-res_m2[:,1])/res_m2[:,0] 
        ## print out mean and standard deviation
        ##print("amu_s from unitary Kaon matching:")
        ##print("amu_s^unit: %f +/- %f\n" % (np.median(amu_s_fin), np.std(amu_s_fin)))
        #
        ## Extract a_kk*am_k
        #par_match = [0.5,-2]
        #res_makk, chi2_makk, pvals_makk = ana.fitting(m_k2,
        #    np.asarray(amu_s)[1:], a_k_fin[:,1:]*m_k_fin[:,1:], par_match, None,
        #    False, False)
        #akk_mk = res_makk[:,1]+res_makk[:,0]*amu_s_fin
        ##print("Extracted m_k*a_kk:")
        ##print("m_k*akk: %f +/- %f\n" % (np.median(akk_mk), np.std(akk_mk)))
        ###plot data
        ##pscat = PdfPages("%s%skk_.pdf" % (tmp_out,"a_"))
        ###fit linear functions
        ##coef,stats = P.polyfit(scat_dat[:,0], scat_dat[:,1]**2,1,None,True,scat_dat[:,2]**2)
        ##print(coef, stats)
        ###linear fitfunction
        ##m_k2 = lambda p, mu : p[0]*mu+p[1]
        ###plot data with linear regression
        ##m_k_sq = ["Kaon scatt. length",  "a_mu_s", "m_K^2","A40.24", "data", ""]
        ##
        ##ana.corr_fct_with_fit(scat_dat[:,0],scat_dat[:,1]**2,scat_dat[:,2]**2,m_k2,
        ##    coef[::-1], [0,2],m_k_sq, pscat, False, False, [0.0185,0.02464])
        ##pscat.close()


# make this script importable, according to the Google Python Style Guide
if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Keyboard Interrupt")
